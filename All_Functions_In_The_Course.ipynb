{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Crawler\n",
    "Starting from a website crawls all the outgoing links, looking for a word/sentence and saving the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from random import random\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def get_links_website(website,keyword_search):\n",
    "    \"\"\"\n",
    "    Finds all the links in the website, and how many times the keyword was found\n",
    "    \"\"\"\n",
    "    try: r = requests.get(website, timeout=100)\n",
    "    except: return 0,[]\n",
    "    if r.ok:\n",
    "        html = bs.BeautifulSoup(r.text, \"lxml\")\n",
    "    else: \n",
    "        print(r.status_code)\n",
    "        return 0,[]\n",
    "    \n",
    "    #Find all snapshots\n",
    "    snapshots = []\n",
    "    all_links = html.find_all(\"a\", href=True)\n",
    "    hits = r.text.count(keyword_search)\n",
    "    return hits,all_links\n",
    "\n",
    "def crawl_one_level(snapshot, link_base, links_snapshot_set,keyword):\n",
    "    \"\"\"\n",
    "    get's the links of the website (link_base) and filter them, keeping the ones that stay in the domain\n",
    "    \"\"\"\n",
    "    hits, links = get_links_website(link_base,keyword)\n",
    "    for link in links:\n",
    "        link_joined = urljoin(link_base, link[\"href\"])\n",
    "        if (snapshot in link_joined) and (link_joined not in links_snapshot_set):\n",
    "            links_snapshot_set.add(link_joined)\n",
    "    return hits, links_snapshot_set\n",
    "\n",
    "\n",
    "def crawl_all_levels(snapshot,keyword):\n",
    "    \"\"\"\n",
    "    crawls the website snapshot, without leaving the domain\n",
    "    \"\"\"\n",
    "    #Get source code\n",
    "    dict_hits = dict()\n",
    "    links_snapshot_set_old = set([snapshot])\n",
    "    new_links = set([snapshot])\n",
    "    while len(new_links) > 0:\n",
    "        for i, link in enumerate(new_links):\n",
    "            sleep(random()*2)            \n",
    "            hits, links_snapshot_set_new = crawl_one_level(snapshot, link,links_snapshot_set_old.copy(),keyword)\n",
    "            print(\"{0}: Link: {1} has the word \\\"{2}\\\" {3} times\".format(i, link,keyword,hits))\n",
    "            dict_hits.update({link: hits})\n",
    "        new_links = links_snapshot_set_new - links_snapshot_set_old\n",
    "        print(\"\\nNew links to follow: \", len(new_links))\n",
    "        links_snapshot_set_old = links_snapshot_set_old | links_snapshot_set_new\n",
    "        \n",
    "    return dict_hits\n",
    "\n",
    "def BFS_keyword(website,keyword,filename):\n",
    "    \"\"\"\n",
    "    calls the script and saves the results to a file\n",
    "    \"\"\"\n",
    "    dict_hits = crawl_all_levels(website,keyword)\n",
    "    \n",
    "    with open(filename, \"w+\") as f:\n",
    "        f.write(\"link\" + \"\\t\" + \"number_hits\" + \"\\n\")\n",
    "        for link in dict_hits:\n",
    "            f.write(link + \"\\t\" + dict_hits[link] + \"\\n\")\n",
    "\n",
    "##USAGE\n",
    "initial_website = \"https://www.amnesty.org/en/\"\n",
    "words_to_find = \"campaign\"\n",
    "file_to_save = \"results_crawling_amnesty.csv\"\n",
    "BFS_keyword(initial_website,words_to_find,file_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# LEXIS NEXIS Crawler\n",
    "Queries a search in lexis nexis, using the CSI (http://www.lexisnexis.com/hottopics/lnacademic/) as the newspaper identifier\n",
    "\n",
    "Disclaimer: We accept no liability for the content of this code, or for the consequences of any actions taken on the basis of the information provided.\n",
    "\n",
    "Code from: http://yc-lexisnexis-scraper.readthedocs.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import os\n",
    "import requests\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "import urllib.parse as urlparse\n",
    "\n",
    "import selenium.common.exceptions\n",
    "import selenium.webdriver\n",
    "import selenium.webdriver.common.desired_capabilities\n",
    "import selenium.webdriver.support.ui\n",
    "from selenium.webdriver.support import expected_conditions\n",
    "\n",
    "\n",
    "class LexisNexisScraper():\n",
    "    \"\"\"\n",
    "    Class for downloading documents given a query string to Lexis Nexis academic (http://www.lexisnexis.com/hottopics/lnacademic/).\n",
    "\n",
    "    Example::\n",
    "\n",
    "            downloader = LexisNexisScraper(mass_download_mode=True)\n",
    "            for (content, (doc_index, doc_count)) in downloader.iter_search_results(6318, 'DATE(=1987)'):\n",
    "                print doc_id\n",
    "\n",
    "     \"\"\"\n",
    "\n",
    "    _RE_STYLESHEET = re.compile(r'\\<STYLE TYPE\\=\\\"text\\/css\\\"\\>(\\<\\!\\-\\-)?(?P<css_string>.+?)(\\-\\-\\>)?\\<\\/STYLE\\>', flags=re.S | re.U | re.I)\n",
    "    _RE_LEXIS_DOC = re.compile(r'\\<DOC NUMBER\\=(?P<docid>\\d+)\\>\\s+\\<DOCFULL\\>(?P<doc>.+?)\\<\\/DOCFULL\\>', flags=re.S | re.U | re.I)\n",
    "\n",
    "    def __init__(self, wait_timeouts=(15, 180), documents_per_download=(250, 500), user_agent_string='Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:25.0) Gecko/20100101 Firefox/25.0', mass_download_mode=False):\n",
    "        \"\"\"\n",
    "        Constructs a downloader object.\n",
    "\n",
    "        :param float,float wait_timeouts: tuple of `(short, long)` where `short` and `long` are the no. of seconds to wait while page elements are loaded (for Webdriver). `long` timeout is used when waiting for LexisNexis to format documents for mass downloads.\n",
    "        :param int,int documents_per_download: a range specifying the number of documents to download each time when using :attr:`mass_download_mode`.\n",
    "        :param bool mass_download_mode: whether to mass download articles using the download link or page through each document one by one and download.\n",
    "        \"\"\"\n",
    "\n",
    "        self._USER_AGENT_STRING = user_agent_string\n",
    "        self._DOCUMENTS_PER_DOWNLOAD = documents_per_download\n",
    "\n",
    "  \n",
    "        self._driver = selenium.webdriver.Firefox()\n",
    "        self._driver.set_window_size(800, 600)\n",
    "\n",
    "        self._short_wait = selenium.webdriver.support.ui.WebDriverWait(self._driver, wait_timeouts[0], poll_frequency=0.05)\n",
    "        self._long_wait = selenium.webdriver.support.ui.WebDriverWait(self._driver, wait_timeouts[1], poll_frequency=1)\n",
    "\n",
    "        self.mass_download_mode_ = mass_download_mode\n",
    "    #end def\n",
    "\n",
    "    def __del__(self):\n",
    "        try: self._driver.quit()\n",
    "        except: pass\n",
    "\n",
    "    def iter_search_results(self, csi, search_query, start_from=1):\n",
    "        \"\"\"\n",
    "        A generator function that executes LexisNexis search query on source data CSI (:attr:`csi`), with query :attr:`search_query` and downloads all documents returned by search.\n",
    "\n",
    "        :param str csi: LexisNexis CSI (see `<http://amdev.net/rpt_download.php>`_ for full list).\n",
    "        :param str search_query: execute search query string.\n",
    "        :param int start_from: document index to start downloading from.\n",
    "        :returns: a tuple `(doc_content, (index, results_count))`, where `doc_content` is the HTML content of the `index`th document, and `results_count` is the number of documents returned by specified search query.\n",
    "        \"\"\"\n",
    "\n",
    "        self._driver.get('http://www.lexisnexis.com/hottopics/lnacademic/?' + urllib.parse.urlencode({'verb': 'sr', 'csi': csi, 'sr': search_query}))\n",
    "        if not self._have_results(): return []\n",
    "\n",
    "        if self.mass_download_mode_: return self._mass_download(start_from)\n",
    "        return self._sequential_download(start_from)\n",
    "    #end def\n",
    "\n",
    "    def _have_results(self):    # todo: kinda slow, due to having wait for multiple timeouts\n",
    "        self._switch_to_frame('main')\n",
    "        if self._wait_for_element('//td[text()[contains(., \\'No Documents Found\\')]]', raise_error=False) is not None: return False\n",
    "        if self._wait_for_element('//frame[@title=\\'Results Content Frame\\']', raise_error=False) is not None: return True\n",
    "        if self._wait_for_element('//frame[@title=\\'Results Document Content Frame\\']', raise_error=False) is not None: return True\n",
    "\n",
    "        raise Exception('Page loaded improperly while checking for results frame.')\n",
    "    #end def\n",
    "\n",
    "    def _mass_download(self, start_from=1):    # Returns documents as a list of strings containing HTML\n",
    "        self._switch_to_frame('navigation')\n",
    "\n",
    "        try: documents_count = int(self._driver.find_element_by_xpath('//form[@name=\\'results_docview_DocumentForm\\']/input[@name=\\'totalDocsInResult\\']').get_attribute('value'))\n",
    "        except: documents_count = -1\n",
    "\n",
    "        def download_sequence(start, end):\n",
    "            docs_left = end - start + 1\n",
    "            cur = start\n",
    "            while docs_left > self._DOCUMENTS_PER_DOWNLOAD[1]:\n",
    "                download_count = random.randint(*self._DOCUMENTS_PER_DOWNLOAD)\n",
    "                yield (cur, cur + download_count - 1)\n",
    "                docs_left -= download_count\n",
    "                cur += download_count\n",
    "            #end while\n",
    "\n",
    "            yield (cur, cur + docs_left - 1)\n",
    "        #end def\n",
    "\n",
    "        def lexis_nexis_download_window_appears(current_handle):\n",
    "            def f(driver):\n",
    "                for handle in driver.window_handles:\n",
    "                    if current_handle != handle:\n",
    "                        driver.switch_to.window(handle)    # switch first to check window title\n",
    "                        if driver.title.endswith('Download Documents'): return True    # this is our new window!\n",
    "                    #end if\n",
    "                #end for\n",
    "\n",
    "                return False\n",
    "            #end def\n",
    "\n",
    "            return f\n",
    "        #end class\n",
    "\n",
    "        for download_start, download_end in download_sequence(start_from, documents_count):\n",
    "            self._switch_to_frame('navigation')\n",
    "\n",
    "            parent_window_handle = self._driver.current_window_handle\n",
    "\n",
    "            # check for download icon and click it\n",
    "            self._wait_for_element('//img[@title=\\'Download Documents\\']').click()\n",
    "\n",
    "            # wait for download window to appear\n",
    "            self._short_wait.until(lexis_nexis_download_window_appears(parent_window_handle))\n",
    "            self._wait_for_element('//img[@title=\\'Download\\']')\n",
    "\n",
    "            # get all the form items\n",
    "            selenium.webdriver.support.ui.Select(self._driver.find_element_by_xpath('//select[@name=\\'delFmt\\']')).select_by_value('QDS_EF_HTML')\n",
    "            selenium.webdriver.support.ui.Select(self._driver.find_element_by_xpath('//select[@name=\\'delView\\']')).select_by_value('GNBFI')\n",
    "            selenium.webdriver.support.ui.Select(self._driver.find_element_by_xpath('//select[@name=\\'delFontType\\']')).select_by_value('COURIER')    # i like courier\n",
    "\n",
    "            search_term_bold = self._driver.find_element_by_xpath('//input[@type=\\'checkbox\\'][@id=\\'termBold\\']')\n",
    "            if not search_term_bold.is_selected(): search_term_bold.click()\n",
    "            doc_new_page = self._driver.find_element_by_xpath('//input[@type=\\'checkbox\\'][@id=\\'docnewpg\\']')\n",
    "            if not doc_new_page.is_selected(): doc_new_page.click()\n",
    "\n",
    "            self._driver.find_element_by_xpath('//input[@type=\\'radio\\'][@id=\\'sel\\']').click()\n",
    "            self._driver.find_element_by_xpath('//input[@type=\\'text\\'][@id=\\'rangetextbox\\']').send_keys('{}-{}'.format(download_start, download_end))\n",
    "\n",
    "            self._driver.find_element_by_xpath('//img[@title=\\'Download\\']').click()\n",
    "\n",
    "            download_url = self._long_wait.until(expected_conditions.presence_of_element_located((selenium.webdriver.common.by.By.XPATH, '//center[@class=\\'suspendbox\\']/p/a'))).get_attribute('href')\n",
    "\n",
    "            # set up cookies and use requests library to do download\n",
    "            cookies = dict([(cookie['name'], cookie['value']) for cookie in self._driver.get_cookies()])\n",
    "            response = requests.get(download_url, cookies=cookies, headers={'User-Agent': self._USER_AGENT_STRING})\n",
    "            html_content = response.text\n",
    "\n",
    "            m = self._RE_STYLESHEET.search(html_content)\n",
    "            css_string = m.group('css_string').strip()\n",
    "\n",
    "            for i, m in enumerate(self._RE_LEXIS_DOC.finditer(html_content)):\n",
    "                page_content = m.group('doc').replace('<!-- Hide XML section from browser', '').replace('-->', '').strip()\n",
    "                page_content = '\\n'.join(['<HTML>', '<HEAD>', '<STYLE TYPE=\\\"text/css\\\">', css_string, '</STYLE>', '</HEAD>', '<BODY>', page_content, '</BODY>', '</HTML>'])\n",
    "\n",
    "                yield (page_content, (download_start + i, documents_count))\n",
    "            #end for\n",
    "\n",
    "            self._driver.close()\n",
    "            self._driver.switch_to.window(parent_window_handle)\n",
    "        #end for\n",
    "    #end def\n",
    "\n",
    "    def _sequential_download(self, start_from=1):\n",
    "        self._switch_to_frame('navigation')\n",
    "        try: documents_count = int(self._driver.find_element_by_xpath('//form[@name=\\'results_docview_DocumentForm\\']/input[@name=\\'totalDocsInResult\\']').get_attribute('value'))\n",
    "        except: documents_count = -1\n",
    "        if documents_count <= 0: return\n",
    "\n",
    "        if start_from > documents_count: return\n",
    "\n",
    "        if documents_count == 1:\n",
    "            self._switch_to_frame('content')\n",
    "            page_content = self._driver.page_source\n",
    "            yield (page_content, (1, 1))\n",
    "            return\n",
    "        #end if\n",
    "\n",
    "        self._switch_to_frame('results')    # go to results list and grab the first link\n",
    "        first_document_url = self._wait_for_element('//td/a[contains(@href, \\'/lnacui2api/results/docview/docview.do\\')]').get_attribute('href')\n",
    "\n",
    "        url_obj = urlparse.urlparse(first_document_url)\n",
    "        qs_dict = dict(urlparse.parse_qsl(url_obj.query))\n",
    "        qs_dict['docNo'] = start_from\n",
    "        doc_url = urlparse.urlunparse((url_obj.scheme, url_obj.netloc, url_obj.path, url_obj.params, urllib.parse.urlencode(qs_dict), url_obj.fragment))\n",
    "        self._driver.get(doc_url)    # jump to the page we want\n",
    "\n",
    "        # qs_dict['RELEVANCE'] = 'BOOLEAN'    # doesnt seem to work\n",
    "        # http://www.lexisnexis.com/lnacui2api/results/docview/docview.do?docLinkInd=true&risb=21_T21153102977&format=GNBFI&sort=RELEVANCE&startDocNo=1&resultsUrlKey=29_T21153102981&cisb=22_T21153102980&treeMax=true&treeWidth=0&csi=6318&docNo=1\n",
    "\n",
    "        for doc_index in range(start_from, documents_count + 1):\n",
    "            self._switch_to_frame('content', in_iframe=False)\n",
    "            page_content = self._driver.page_source\n",
    "            yield (page_content, (doc_index, documents_count))\n",
    "\n",
    "            self._switch_to_frame('navigation', in_iframe=False)\n",
    "            next_page_elem = self._wait_for_element('//img[@title=\\'View next document\\']', raise_error=False)\n",
    "            if next_page_elem is None:\n",
    "                if doc_index != documents_count:\n",
    "                    raise Exception('Next page icon could not be found: doc_index={}, documents_count={}'.format(doc_index, documents_count))\n",
    "            else: next_page_elem.click()\n",
    "        #end while\n",
    "    #end def\n",
    "\n",
    "    def _switch_to_frame(self, frame_name, in_iframe=True):\n",
    "        self._driver.switch_to.default_content()\n",
    "\n",
    "        if in_iframe:\n",
    "            frame = self._safe_wait(expected_conditions.frame_to_be_available_and_switch_to_it('mainFrame'))\n",
    "            if not frame: raise SwitchFrameException(frame_name)\n",
    "        #end if\n",
    "\n",
    "        try:\n",
    "            if frame_name == 'main': return frame\n",
    "            elif frame_name == 'results': frame = self._wait_for_element('//frame[@title=\\'Results Content Frame\\']')\n",
    "            elif frame_name == 'navigation': frame = self._wait_for_element('//frame[@title=\\'Results Navigation Frame\\']')\n",
    "            elif frame_name == 'content': frame = self._wait_for_element('//frame[@title=\\'Results Document Content Frame\\']')\n",
    "        except selenium.common.exceptions.TimeoutException:\n",
    "            raise SwitchFrameException(frame_name)\n",
    "\n",
    "        self._safe_wait(expected_conditions.frame_to_be_available_and_switch_to_it(frame))\n",
    "\n",
    "        return frame\n",
    "    #end def\n",
    "\n",
    "    def _safe_wait(self, poll_func):\n",
    "        try: return self._short_wait.until(poll_func)\n",
    "        except selenium.common.exceptions.TimeoutException: return None\n",
    "    #end def\n",
    "\n",
    "    def _wait_for_element(self, xpath, raise_error=True):\n",
    "        elem = self._safe_wait(expected_conditions.presence_of_element_located((selenium.webdriver.common.by.By.XPATH, xpath)))\n",
    "        if raise_error and elem is None: raise selenium.common.exceptions.TimeoutException(msg='XPath \\'{}\\' presence wait timeout.'.format(xpath))\n",
    "        return elem\n",
    "    #end def\n",
    "#end class\n",
    "\n",
    "\n",
    "class SwitchFrameException(Exception):\n",
    "    \"\"\"\n",
    "    Exception class when we are unable to load the require page properly.\n",
    "    This is usually due to\n",
    "    #. Page taking too long to load. This happens sometimes when loading LexisNexis for the first time.\n",
    "    #. Improper page loading.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, frame_name): self.frame_name = frame_name\n",
    "\n",
    "    def __str__(self): return 'Exception while switching to frame \\'{}\\'.'.format(self.frame_name)\n",
    "#end class\n",
    "\n",
    "import bs4 as bs\n",
    "def scrape_news(csi,init_date,end_date,search,path=\"./\"):\n",
    "    \"\"\"\n",
    "    call the code from that other person, saves the results (one file = one article)\n",
    "    \"\"\"\n",
    "    import os      \n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    query = \"{0} AND Date(geq({1}) AND leq({2}))\".format(search,init_date,end_date)\n",
    "\n",
    "    scrapper = LexisNexisScraper(mass_download_mode=True)\n",
    "\n",
    "    for (content, (doc_index, doc_count)) in scrapper.iter_search_results(csi,query):\n",
    "        with open(\"{0}/{1}_{2}_{3}.csv\".format(path,search.replace(\" \",\"_\").replace(\"/\",\"-\"),doc_index, doc_count), \"w+\") as fOut:\n",
    "            fOut.write(content)    \n",
    "\n",
    "\n",
    "def format_new(content):\n",
    "    \"\"\"\n",
    "    finds the article as a paragraph with class c8 or c9, and the other parts that we want too\n",
    "    \"\"\"\n",
    "    html = bs.BeautifulSoup(content, \"lxml\")\n",
    "    #print(html.prettify())\n",
    "    new = \" \".join([_.text for _ in html.find_all(\"p\",{\"class\": [\"c8\",\"c9\"]}) if len(_.text) > 100])\n",
    "    other_parts = html.find_all(\"p\",{\"class\": \"c1\"})\n",
    "    date = other_parts[2].text\n",
    "    newspaper = other_parts[1].text\n",
    "    print(date)\n",
    "    return date,newspaper,new\n",
    "\n",
    "def format_all_files(path,file_out_name,search):\n",
    "    \"\"\"\n",
    "    format the files, saving date, newspaper and article and saves the result\n",
    "    \"\"\"\n",
    "    import os      \n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(\"./data/{0}\".format(file_out_name),\"w+\") as fOut:        \n",
    "        for file in sorted(os.listdir(path)):\n",
    "            if not search.replace(\" \",\"_\") in file: continue\n",
    "            print(file)\n",
    "            date,newspaper,new = format_new(open(\"./data/news_raw/{0}\".format(file)))    \n",
    "            if len(new) > 100:\n",
    "                fOut.write(\"{0}\\t{1}\\t{2}\\n\".format(date,newspaper,new))\n",
    "            #break\n",
    "            \n",
    "##USAGE\n",
    "#CSI = Newspaper identifier: http://www.lexisnexis.com/hottopics/lnacademic/\n",
    "csi = \"166103\"\n",
    "\n",
    "#Initial and final dates\n",
    "init_date = \"11/20/2015\" #format: mm/dd/yyyy\n",
    "end_date = \"1/20/2016\"  #format: mm/dd/yyyy\n",
    "\n",
    "#Query (Use AND, OR, and parenthesis), saves the results to /data/news_raw\n",
    "search = \"Pablo Iglesias AND Podemos\"\n",
    "scrape_news(csi,init_date,end_date,search,path=\"./data/news_raw\")\n",
    "\n",
    "#Where to save the results, formatting it as date, newspaper, article\n",
    "path = \"./data/news_raw/\"\n",
    "file_out_name = \"Pablo_Iglesias_Podemos.csv\"\n",
    "format_all_files(path,file_out_name,search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other useful stuff\n",
    "- Linke histogram in linear and log scale\n",
    "- Correlation\n",
    "- Cross-correlation\n",
    "- Compare samples (ttest, MWU,anova)\n",
    "- Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_line_hist(x,minValue,maxValue,nbins=10,logscale=False):\n",
    "    \"\"\"\n",
    "    make a line histogram (instead of bars, just the top line)\n",
    "    \"\"\"\n",
    "    if logscale:\n",
    "        freqs,bins = np.histogram(x,bins=np.logspace(np.log10(minValue),np.log10(maxValue),nbins),normed=True) \n",
    "    else:\n",
    "        freqs,bins = np.histogram(x,bins=np.linspace(minValue,maxValue,nbins),normed=True)\n",
    "        \n",
    "    \n",
    "    plt.plot((bins[:-1]+bins[1:])/2,freqs)\n",
    "    return freqs\n",
    "\n",
    "def correlation(x,y,type=\"pearson\"):\n",
    "    from scipy.stats import pearsonr, spearmanr\n",
    "    if type == \"pearson\":\n",
    "        corr,p_value = pearsonr(x,y)\n",
    "    else:\n",
    "        corr,p_value = spearmanr(x,y)\n",
    "    \n",
    "    print(\"Correlation: {0:2.2g}. P-value: {1:2.5f}\".format(corr,p_value))\n",
    "    return corr,p_value\n",
    "    \n",
    "def cross_correlation(x,y,time):\n",
    "    \"\"\"\n",
    "    Calculates the normalized cross-correlation and plots it\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    modeC = \"same\"\n",
    "    x = (x - np.mean(x))/np.std(x)\n",
    "    y =  (y - np.mean(y))/np.std(y)\n",
    "\n",
    "    timeInt = np.diff(time).mean().days\n",
    "    numPoints = len(x)\n",
    "    fig = plt.figure(figsize=(6,3.5))        \n",
    "    d = np.correlate(y,x,modeC)\n",
    "\n",
    "    plt.plot([0,0],[-0.5,1],color=\"grey\")\n",
    "    plt.xlabel(\"Lag\")\n",
    "    plt.ylabel(\"Correlation\")\n",
    "    plt.plot(np.linspace(len(x)/2*timeInt,-len(x)/2*timeInt,len(x)),d/numPoints)\n",
    "    plt.show()\n",
    "\n",
    "def compare_samples(populations,parametric=False):\n",
    "    \"\"\"\n",
    "    check if the samples come from the same population or not\n",
    "    \"\"\"\n",
    "    from scipy.stats import mannwhitneyu, ttest_ind, f_oneway, kruskal\n",
    "    if len(populations) == 2:\n",
    "        if parametric:\n",
    "            stat, p_value = mannwhitneyu(*populations)\n",
    "        else:\n",
    "            stat, p_value = mannwhitneyu(*populations)\n",
    "    if len(populations) > 2:\n",
    "        if parametric:\n",
    "            stat, p_value = f_oneway(*populations)\n",
    "        else:\n",
    "            stat, p_value = kruskal(*populations)        \n",
    "            \n",
    "    print(\"P-value: {0:2.5f}\".format(p_value))\n",
    "    return p_value\n",
    "\n",
    "def linear_regression(vectors_X,vector_y,variables_names=\"ABCDEFG\",formula=\"Y ~ A * B\"):\n",
    "    \"\"\"\n",
    "    linear_regression\n",
    "    vectors_X: list of the vectors X\n",
    "    vector_y: independent variable\n",
    "    variables_names: whatever you want to name the variables in X \n",
    "    formula: formulat to use\n",
    "    \"\"\"\n",
    "    #linear regression\n",
    "    import pandas as pd\n",
    "    import statsmodels.formula.api as sm\n",
    "    d = dict(zip(variables_names,vectors_X))\n",
    "    d[\"Y\"] = vector_y\n",
    "             \n",
    "    df = pd.DataFrame(d)\n",
    "    result = sm.ols(formula=formula, data=df).fit()\n",
    "    print(result.summary())\n",
    "    \n",
    "    return result.params\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
